# BIGDATA_PROJECT - Há»‡ thá»‘ng xá»­ lÃ½ dá»¯ liá»‡u báº¥t Ä‘á»™ng sáº£n Real-time

Dá»± Ã¡n xÃ¢y dá»±ng pipeline Big Data thu tháº­p vÃ  xá»­ lÃ½ dá»¯ liá»‡u báº¥t Ä‘á»™ng sáº£n tá»« nhatot.com.

## ğŸ“ Kiáº¿n trÃºc há»‡ thá»‘ng

```
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚   Data Source      â”‚
           â”‚ nhatot.com Web     â”‚
           â”‚ Scraping/ Crawlingâ”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”
           â”‚ Data Ingestion     â”‚
           â”‚ (Kafka Producer)   â”‚
           â”‚ Stream of Listings â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚ Stream Processing â”‚ 
           â”‚ (Apache Spark     â”‚
           â”‚ Structured Streaming) â”‚
           â”‚ - Clean data      â”‚
           â”‚ - Transformations â”‚
           â”‚ - UDFs, Aggregatesâ”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                          â”‚
        â–¼                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Batch Storage     â”‚       â”‚ NoSQL Database    â”‚
â”‚ HDFS              â”‚       â”‚ MongoDB           â”‚
â”‚ Raw & Processed   â”‚       â”‚ Fast querying &   â”‚
â”‚ Data              â”‚       â”‚ analytics         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                          â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â–¼
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚ Visualization â”‚
               â”‚ Dashboards    â”‚
               â”‚ (Grafana /    â”‚
               â”‚ Superset)     â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Cáº¥u trÃºc Project

```
BIGDATA_PROJECT/
â”œâ”€â”€ CrawlData.py              # Module crawl dá»¯ liá»‡u tá»« nhatot.com API
â”œâ”€â”€ kafka_producer.py         # Kafka Producer - stream data vÃ o Kafka âœ…
â”œâ”€â”€ kafka_consumer_test.py    # Test consumer Ä‘á»ƒ kiá»ƒm tra data trong Kafka
â”œâ”€â”€ test_spark_streaming.py   # [Cáº¦N TRIá»‚N KHAI] Spark Structured Streaming
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ data_input/house/         # Dá»¯ liá»‡u Ä‘Ã£ crawl (backup)
â”‚   â””â”€â”€ 2025-12-12/          # 36+ JSON files
â””â”€â”€ README.md                 # File nÃ y
```

## ğŸ”§ MÃ´i trÆ°á»ng cáº§n thiáº¿t

### ÄÃ£ cÃ i Ä‘áº·t trÃªn WSL2:
- **Java 11** (`/usr/lib/jvm/java-11-openjdk-amd64`)
- **Kafka 3.6.1** (`/usr/local/kafka`)
- **Hadoop** (`/usr/local/hadoop`)
- **Spark** - Cáº§n cÃ i Ä‘áº·t cho Stream Processing

### Python Libraries:
```bash
pip install -r requirements.txt
# CÃ i Ä‘áº·t: requests, kafka-python, python-dotenv
```

---

##  Cáº¥u hÃ¬nh mÃ´i trÆ°á»ng (.env)

### BÆ°á»›c 1: Táº¡o file .env
```bash
# Copy file template
cp .env.example .env
```

### BÆ°á»›c 2: Láº¥y IP cá»§a WSL2
Trong WSL2 terminal, cháº¡y:
```bash
hostname -I
# VÃ­ dá»¥ káº¿t quáº£: 172.27.34.172
```

### BÆ°á»›c 3: Cáº­p nháº­t file .env

Ná»™i dung file `.env`:
```env
# IP cá»§a WSL2 (thay báº±ng IP thá»±c táº¿ cá»§a báº¡n)
WSL2_IP=172.27.34.172

# Kafka Configuration
KAFKA_PORT=9092
KAFKA_TOPIC=house-listings

# Producer Settings
CRAWL_LIMIT=50
BATCH_SIZE=10
```

**LÆ°u Ã½:** 
- Má»—i khi restart Windows, IP cá»§a WSL2 cÃ³ thá»ƒ thay Ä‘á»•i, cáº§n cáº­p nháº­t láº¡i

---

## Pháº§n 1: DATA INGESTION

### Khá»Ÿi Ä‘á»™ng Kafka Cluster

#### Terminal WSL 1 - Zookeeper:
```bash
cd /usr/local/kafka
bin/zookeeper-server-start.sh config/zookeeper.properties
```

#### Terminal WSL 2 - Kafka:
```bash
cd /usr/local/kafka
bin/kafka-server-start.sh config/server.properties
```

#### Kiá»ƒm tra services:
```bash
jps
# Pháº£i tháº¥y:
# XXXX QuorumPeerMain (Zookeeper)
# YYYY Kafka
```

### Táº¡o Kafka Topic

```bash
kafka-topics.sh --create --topic house-listings \
  --bootstrap-server localhost:9092 \
  --partitions 3 \
  --replication-factor 1

# Kiá»ƒm tra topic
kafka-topics.sh --list --bootstrap-server localhost:9092
kafka-topics.sh --describe --topic house-listings --bootstrap-server localhost:9092
```

###  Cháº¡y Producer (tá»« Windows)

#### BÆ°á»›c 1: CÃ i Ä‘áº·t dependencies
```bash
pip install -r requirements.txt
```

#### BÆ°á»›c 2: Äáº£m báº£o file .env Ä‘Ã£ cáº¥u hÃ¬nh Ä‘Ãºng
```bash
# Kiá»ƒm tra file .env cÃ³ WSL2_IP Ä‘Ãºng khÃ´ng
cat .env  # WSL2
# hoáº·c
type .env  # Windows CMD
# hoáº·c
Get-Content .env  # PowerShell
```

#### BÆ°á»›c 3: Cháº¡y Producer
```bash
python kafka_producer.py
```

**Káº¿t quáº£ mong Ä‘á»£i:**
```
[CONFIG] Kafka Bootstrap: 172.27.34.172:9092
[CONFIG] Topic: house-listings
[CONFIG] Crawl Limit: 50, Batch Size: 10
[âœ“] Kafka connection successful!
[INFO] Kafka Producer initialized - Topic: house-listings
[INFO] Starting to crawl and stream 50 listings...
[INFO] Láº¥y Ä‘Æ°á»£c 50 list_id.
[1/50] Processing ID: 129940480
[SUCCESS] Sent ID: 129940480 â†’ Topic: house-listings, Partition: 0, Offset: 0
...
[COMPLETED] Sent 50/50 listings to Kafka
```

**Náº¿u lá»—i káº¿t ná»‘i:**
- Kiá»ƒm tra Kafka Ä‘ang cháº¡y: `jps` trong WSL2
- Cáº­p nháº­t láº¡i `WSL2_IP` trong file `.env`
- Test connection: `Test-NetConnection -ComputerName <WSL2_IP> -Port 9092`

### Kiá»ƒm tra dá»¯ liá»‡u trong Kafka

#### Tá»« WSL2:
```bash
# Xem 5 messages Ä‘áº§u
kafka-console-consumer.sh \
  --bootstrap-server localhost:9092 \
  --topic house-listings \
  --from-beginning \
  --max-messages 5
```

#### Tá»« Windows:
```bash
python kafka_consumer_test.py
```

### Format dá»¯ liá»‡u trong Kafka

Má»—i message cÃ³ format JSON:
```json
{
  "id": 129940480,
  "title": "BÃ¡n cÄƒn há»™ 2PN...",
  "description": "...",
  "price": 3650000000,
  "area_m2": 123,
  "price_per_m2": 29674796.75,
  "region": "HÃ  Ná»™i",
  "district": "Huyá»‡n Gia LÃ¢m",
  "ward": "Thá»‹ tráº¥n TrÃ¢u Quá»³",
  "street": "YÃªn ViÃªn",
  "lat": 21.02097,
  "lng": 105.93817,
  "property_type": null,
  "category": 1010,
  "post_time": 1765506961000,
  "images": 12,
  "crawl_timestamp": 1735306123.456,
  "source": "nhatot.com"
}
```

---

## Pháº§n 2: STREAM PROCESSING vá»›i Apache Spark

### Má»¥c tiÃªu
XÃ¢y dá»±ng Spark Structured Streaming Ä‘á»ƒ:
1. **Äá»c dá»¯ liá»‡u real-time** tá»« Kafka topic `house-listings`
2. **LÃ m sáº¡ch vÃ  transform** dá»¯ liá»‡u
3. **TÃ­nh toÃ¡n aggregations** (giÃ¡ trung bÃ¬nh, sá»‘ lÆ°á»£ng tin Ä‘Äƒng theo khu vá»±c)
4. **LÆ°u trá»¯** vÃ o HDFS (Parquet) vÃ  MongoDB

---

### BÆ°á»›c 1: CÃ i Ä‘áº·t Apache Spark (WSL2)

#### 1.1. Download vÃ  cÃ i Ä‘áº·t Spark
```bash
cd /usr/local

# Download Spark 3.5.3 (phiÃªn báº£n má»›i nháº¥t, tÆ°Æ¡ng thÃ­ch vá»›i Hadoop 3.x)
# Náº¿u link khÃ´ng hoáº¡t Ä‘á»™ng, kiá»ƒm tra phiÃªn báº£n má»›i táº¡i: https://spark.apache.org/downloads.html
sudo wget https://dlcdn.apache.org/spark/spark-3.5.3/spark-3.5.3-bin-hadoop3.tgz

# Hoáº·c tá»« archive:
# sudo wget https://archive.apache.org/dist/spark/spark-3.5.3/spark-3.5.3-bin-hadoop3.tgz

# Giáº£i nÃ©n
sudo tar -xzf spark-3.5.3-bin-hadoop3.tgz
sudo mv spark-3.5.3-bin-hadoop3 spark

# XÃ³a file táº£i vá»
sudo rm spark-3.5.3-bin-hadoop3.tgz

# PhÃ¢n quyá»n
sudo chown -R $USER:$USER /usr/local/spark
```

#### 1.2. Cáº¥u hÃ¬nh biáº¿n mÃ´i trÆ°á»ng
```bash
# Má»Ÿ file .bashrc
nano ~/.bashrc

# ThÃªm cÃ¡c dÃ²ng sau vÃ o cuá»‘i file:
export SPARK_HOME=/usr/local/spark
export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
export PYSPARK_PYTHON=python3

# LÆ°u file (Ctrl+O, Enter, Ctrl+X)

# Load láº¡i cáº¥u hÃ¬nh
source ~/.bashrc
```

#### 1.3. Kiá»ƒm tra cÃ i Ä‘áº·t
```bash
# Kiá»ƒm tra Spark version
spark-submit --version

# Kiá»ƒm tra PySpark
pyspark --version

# Test PySpark shell (Ctrl+D Ä‘á»ƒ thoÃ¡t)
pyspark
```

**Káº¿t quáº£ mong Ä‘á»£i:**
```
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.5.3
      /_/
```

---

### BÆ°á»›c 2: CÃ i Ä‘áº·t Python Dependencies

**LÆ°u Ã½:** Ubuntu 22.04+ yÃªu cáº§u sá»­ dá»¥ng virtual environment Ä‘á»ƒ trÃ¡nh lá»—i `externally-managed-environment`.

#### 2.1. CÃ i Ä‘áº·t python3-venv
```bash
sudo apt install python3-venv python3-full -y
```

#### 2.2. Táº¡o vÃ  kÃ­ch hoáº¡t virtual environment
```bash
# Táº¡o venv trong home directory
cd ~
python3 -m venv spark-venv

# KÃ­ch hoáº¡t venv (dáº¥u (spark-venv) sáº½ xuáº¥t hiá»‡n á»Ÿ Ä‘áº§u prompt)
source ~/spark-venv/bin/activate
```

#### 2.3. CÃ i Ä‘áº·t packages trong venv
```bash
# CÃ i Ä‘áº·t PySpark vÃ  PyMongo
pip install pyspark pymongo

# Verify cÃ i Ä‘áº·t thÃ nh cÃ´ng
python -c "import pyspark; print(pyspark.__version__)"
python -c "import pymongo; print(pymongo.__version__)"
```

#### 2.4. Cáº¥u hÃ¬nh PySpark sá»­ dá»¥ng venv Python
```bash
# Má»Ÿ file .bashrc
nano ~/.bashrc

# TÃ¬m dÃ²ng: export PYSPARK_PYTHON=python3
# Thay báº±ng (thay 'donglam' báº±ng username cá»§a báº¡n):
export PYSPARK_PYTHON=/home/donglam/spark-venv/bin/python3
export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.9.7-src.zip:$PYTHONPATH

# LÆ°u file (Ctrl+O, Enter, Ctrl+X)

# Load láº¡i cáº¥u hÃ¬nh
source ~/.bashrc
```

#### 2.5. Test cÃ i Ä‘áº·t
```bash
# Äáº£m báº£o venv Ä‘ang active
source ~/spark-venv/bin/activate

# Test PySpark
pyspark --version
```

**Káº¿t quáº£ mong Ä‘á»£i:**
```
Python 3.x.x
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.5.3
      /_/
```

**LÆ°u Ã½ quan trá»ng:** Má»—i khi má»Ÿ terminal má»›i Ä‘á»ƒ cháº¡y Spark, nhá»› activate venv:
```bash
source ~/spark-venv/bin/activate
```p

---

### BÆ°á»›c 3: CÃ i Ä‘áº·t vÃ  khá»Ÿi Ä‘á»™ng MongoDB (WSL2)

#### 3.1. CÃ i Ä‘áº·t MongoDB
```bash
# Import MongoDB GPG key
curl -fsSL https://www.mongodb.org/static/pgp/server-7.0.asc | \
   sudo gpg -o /usr/share/keyrings/mongodb-server-7.0.gpg --dearmor

# ThÃªm MongoDB repository
echo "deb [ arch=amd64,arm64 signed-by=/usr/share/keyrings/mongodb-server-7.0.gpg ] https://repo.mongodb.org/apt/ubuntu jammy/mongodb-org/7.0 multiverse" | \
   sudo tee /etc/apt/sources.list.d/mongodb-org-7.0.list

# Update vÃ  cÃ i Ä‘áº·t
sudo apt-get update
sudo apt-get install -y mongodb-org
```

#### 3.2. Khá»Ÿi Ä‘á»™ng MongoDB
```bash
# Start MongoDB service
sudo systemctl start mongod

# Enable auto-start
sudo systemctl enable mongod

# Kiá»ƒm tra status
sudo systemctl status mongod

# Test connection
mongosh
# Trong mongosh shell:
show dbs
exit
```

#### 3.3. Táº¡o database vÃ  collection
```bash
mongosh
```

Trong MongoDB shell:
```javascript
// Táº¡o database
use bigdata_houses

// Táº¡o collection vá»›i index
db.createCollection("listings")

// Táº¡o index cho performance
db.listings.createIndex({ "id": 1 }, { unique: true })
db.listings.createIndex({ "region": 1, "district": 1 })
db.listings.createIndex({ "price": 1 })
db.listings.createIndex({ "crawl_timestamp": -1 })

// Kiá»ƒm tra
show collections
db.listings.getIndexes()

exit
```

---

### BÆ°á»›c 4: Khá»Ÿi Ä‘á»™ng HDFS

#### 4.0. Cáº¥u hÃ¬nh SSH vÃ  Hadoop (cháº¡y láº§n Ä‘áº§u tiÃªn)

**A. CÃ i Ä‘áº·t vÃ  cáº¥u hÃ¬nh SSH:**
```bash
# 1. CÃ i Ä‘áº·t SSH server
sudo apt install openssh-server -y

# 2. Khá»Ÿi Ä‘á»™ng SSH service
sudo service ssh start

# 3. Kiá»ƒm tra SSH Ä‘ang cháº¡y
sudo service ssh status

# 4. Táº¡o SSH key (cho passwordless SSH)
ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa

# 5. Copy public key vÃ o authorized_keys
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
chmod 0600 ~/.ssh/authorized_keys

# 6. Test SSH Ä‘áº¿n localhost
ssh localhost
# GÃµ "yes" khi Ä‘Æ°á»£c há»i, sau Ä‘Ã³ gÃµ "exit" Ä‘á»ƒ thoÃ¡t
```

**B. Cáº¥u hÃ¬nh Hadoop core-site.xml:**
```bash
# Backup file cÅ©
sudo cp /usr/local/hadoop/etc/hadoop/core-site.xml /usr/local/hadoop/etc/hadoop/core-site.xml.backup

# Sá»­a file
sudo nano /usr/local/hadoop/etc/hadoop/core-site.xml
```

Ná»™i dung file `core-site.xml`:
```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/tmp/hadoop-${user.name}</value>
    </property>
</configuration>
```

**C. Kiá»ƒm tra hdfs-site.xml:**
```bash
sudo nano /usr/local/hadoop/etc/hadoop/hdfs-site.xml
```

Ná»™i dung file `hdfs-site.xml`:
```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
</configuration>
```

#### 4.1. Format NameNode (chá»‰ láº§n Ä‘áº§u tiÃªn)
```bash
# Chá»‰ cháº¡y láº§n Ä‘áº§u hoáº·c khi cáº§n reset HDFS
hdfs namenode -format -force
```

**Káº¿t quáº£ mong Ä‘á»£i:**
```
...
INFO namenode.FSImageFormatProtobuf: Image file ... saved in 0 seconds
INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0
INFO namenode.FSNamesystem: Stopping services started for active state
SHUTDOWN_MSG: Shutting down NameNode at ...
```

#### 4.2. Khá»Ÿi Ä‘á»™ng HDFS
```bash
# Start HDFS services
start-dfs.sh

# Kiá»ƒm tra services
jps
# Pháº£i tháº¥y:
# - NameNode
# - DataNode
# - SecondaryNameNode
# - QuorumPeerMain (Zookeeper)
# - Kafka
```

**LÆ°u Ã½:** 
- Warning "Unable to load native-hadoop library" lÃ  bÃ¬nh thÆ°á»ng trÃªn WSL2, khÃ´ng áº£nh hÆ°á»Ÿng hoáº¡t Ä‘á»™ng
- Warning "Cannot set priority" cÅ©ng khÃ´ng áº£nh hÆ°á»Ÿng

#### 4.3. Táº¡o thÆ° má»¥c trong HDFS
```bash
# Táº¡o thÆ° má»¥c cho dá»¯ liá»‡u
hdfs dfs -mkdir -p /bigdata/house-listings/raw
hdfs dfs -mkdir -p /bigdata/house-listings/processed
hdfs dfs -mkdir -p /bigdata/checkpoints

# Kiá»ƒm tra
hdfs dfs -ls /bigdata
hdfs dfs -ls /bigdata/house-listings
```

#### 4.4. Truy cáº­p HDFS Web UI
Má»Ÿ trÃ¬nh duyá»‡t: http://localhost:9870

---

### ğŸ’» BÆ°á»›c 5: Táº¡o Spark Streaming Script

Táº¡o file `spark_streaming_consumer.py`:

```python
"""
Spark Structured Streaming Consumer
Äá»c dá»¯ liá»‡u tá»« Kafka, xá»­ lÃ½ vÃ  ghi vÃ o HDFS + MongoDB
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
import os

# Cáº¥u hÃ¬nh
KAFKA_BOOTSTRAP = os.getenv('WSL2_IP', 'localhost') + ':9092'
KAFKA_TOPIC = 'house-listings'
MONGODB_URI = "mongodb://localhost:27017/bigdata_houses.listings"
HDFS_PATH = "hdfs://localhost:9000/bigdata/house-listings"
CHECKPOINT_PATH = "file:///tmp/spark-checkpoints"  # DÃ¹ng local thay vÃ¬ HDFS

print(f"[CONFIG] Kafka: {KAFKA_BOOTSTRAP}")
print(f"[CONFIG] Topic: {KAFKA_TOPIC}")
print(f"[CONFIG] MongoDB: {MONGODB_URI}")
print(f"[CONFIG] HDFS: {HDFS_PATH}")

# 1. Khá»Ÿi táº¡o Spark Session
spark = SparkSession.builder \
    .appName("HouseListingsStreaming") \
    .config("spark.jars.packages", 
            "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3,"
            "org.mongodb.spark:mongo-spark-connector_2.12:10.4.0") \
    .config("spark.mongodb.write.connection.uri", MONGODB_URI) \
    .config("spark.sql.streaming.checkpointLocation", CHECKPOINT_PATH) \
    .getOrCreate()

spark.sparkContext.setLogLevel("WARN")

print("[INFO] Spark Session initialized")

# 2. Define schema cho dá»¯ liá»‡u
schema = StructType([
    StructField("id", LongType(), True),
    StructField("title", StringType(), True),
    StructField("description", StringType(), True),
    StructField("price", LongType(), True),
    StructField("area_m2", DoubleType(), True),
    StructField("price_per_m2", DoubleType(), True),
    StructField("region", StringType(), True),
    StructField("district", StringType(), True),
    StructField("ward", StringType(), True),
    StructField("street", StringType(), True),
    StructField("lat", DoubleType(), True),
    StructField("lng", DoubleType(), True),
    StructField("property_type", StringType(), True),
    StructField("category", IntegerType(), True),
    StructField("post_time", LongType(), True),
    StructField("images", IntegerType(), True),
    StructField("crawl_timestamp", DoubleType(), True),
    StructField("source", StringType(), True)
])

# 3. Äá»c stream tá»« Kafka
print("[INFO] Connecting to Kafka...")
kafka_df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", KAFKA_BOOTSTRAP) \
    .option("subscribe", KAFKA_TOPIC) \
    .option("startingOffsets", "latest") \
    .option("failOnDataLoss", "false") \
    .load()

print("[INFO] Connected to Kafka successfully")

# 4. Parse JSON tá»« Kafka message
parsed_df = kafka_df.select(
    from_json(col("value").cast("string"), schema).alias("data"),
    col("timestamp").alias("kafka_timestamp")
).select("data.*", "kafka_timestamp")

# 5. Data Cleaning & Transformations
cleaned_df = parsed_df \
    .filter(col("price").isNotNull()) \
    .filter(col("price") > 0) \
    .filter(col("area_m2").isNotNull()) \
    .filter(col("area_m2") > 0) \
    .withColumn("processing_time", current_timestamp()) \
    .withColumn("price_billion", round(col("price") / 1000000000, 2)) \
    .withColumn("price_category", 
                when(col("price") < 1000000000, "< 1 tá»·")
                .when(col("price") < 3000000000, "1-3 tá»·")
                .when(col("price") < 5000000000, "3-5 tá»·")
                .when(col("price") < 10000000000, "5-10 tá»·")
                .otherwise("> 10 tá»·")) \
    .withColumn("area_category",
                when(col("area_m2") < 50, "< 50mÂ²")
                .when(col("area_m2") < 100, "50-100mÂ²")
                .when(col("area_m2") < 200, "100-200mÂ²")
                .otherwise("> 200mÂ²"))

print("[INFO] Data cleaning configured")

# 6. Write to HDFS (Parquet format) - Raw Data
query_hdfs_raw = cleaned_df.writeStream \
    .outputMode("append") \
    .format("parquet") \
    .option("path", f"{HDFS_PATH}/raw") \
    .option("checkpointLocation", f"{CHECKPOINT_PATH}/raw") \
    .trigger(processingTime="30 seconds") \
    .start()

print(f"[INFO] Writing to HDFS: {HDFS_PATH}/raw")

# 7. Aggregations - TÃ­nh giÃ¡ trung bÃ¬nh theo quáº­n
agg_by_district = cleaned_df \
    .groupBy(
        window(col("processing_time"), "5 minutes"),
        col("region"),
        col("district")
    ) \
    .agg(
        count("*").alias("total_listings"),
        avg("price").alias("avg_price"),
        min("price").alias("min_price"),
        max("price").alias("max_price"),
        avg("area_m2").alias("avg_area"),
        avg("price_per_m2").alias("avg_price_per_m2")
    ) \
    .withColumn("window_start", col("window.start")) \
    .withColumn("window_end", col("window.end")) \
    .drop("window")

# Write aggregations to HDFS
# Note: Táº¡m comment vÃ¬ Parquet khÃ´ng há»— trá»£ Complete mode
# Sáº½ cáº§n thÃªm watermark vÃ  dÃ¹ng append mode Ä‘á»ƒ fix
# query_hdfs_agg = agg_by_district.writeStream \
#     .outputMode("complete") \
#     .format("parquet") \
#     .option("path", f"{HDFS_PATH}/aggregations") \
#     .option("checkpointLocation", f"{CHECKPOINT_PATH}/aggregations") \
#     .trigger(processingTime="1 minute") \
#     .start()

# print(f"[INFO] Writing aggregations to HDFS: {HDFS_PATH}/aggregations")

# 8. Write to MongoDB
def write_to_mongodb(batch_df, batch_id):
    """Ghi batch vÃ o MongoDB (upsert Ä‘á»ƒ trÃ¡nh duplicates)"""
    try:
        # Äáº¿m sá»‘ records trÆ°á»›c khi insert
        total_records = batch_df.count()
        
        batch_df.write \
            .format("mongodb") \
            .mode("append") \
            .option("database", "bigdata_houses") \
            .option("collection", "listings") \
            .option("replaceDocument", "true") \
            .option("idFieldList", "id") \
            .option("ordered", "false") \
            .save()
        print(f"[SUCCESS] Batch {batch_id}: Processed {total_records} records to MongoDB (upsert mode)")
    except Exception as e:
        print(f"[ERROR] Batch {batch_id}: Failed to write to MongoDB: {e}")

query_mongo = cleaned_df.writeStream \
    .foreachBatch(write_to_mongodb) \
    .option("checkpointLocation", f"{CHECKPOINT_PATH}/mongodb") \
    .trigger(processingTime="30 seconds") \
    .start()

print("[INFO] Writing to MongoDB configured")

# 9. Console output Ä‘á»ƒ debug (optional)
query_console = cleaned_df.select(
    "id", "title", "price_billion", "area_m2", 
    "district", "price_category", "area_category"
).writeStream \
    .outputMode("append") \
    .format("console") \
    .option("truncate", "false") \
    .option("numRows", 5) \
    .trigger(processingTime="1 minute") \
    .start()

print("[INFO] Console output enabled")

# 10. Await termination
print("\n" + "="*80)
print("ğŸš€ Spark Streaming Started Successfully!")
print("="*80)
print(f"ğŸ“Š Monitoring:")
print(f"   - Spark UI: http://localhost:4040")
print(f"   - HDFS UI: http://localhost:9870")
print(f"   - MongoDB: mongosh â†’ use bigdata_houses â†’ db.listings.find().limit(5)")
print("="*80)
print("\nâ³ Waiting for data from Kafka topic 'house-listings'...")
print("Press Ctrl+C to stop\n")

try:
    query_hdfs_raw.awaitTermination()
except KeyboardInterrupt:
    print("\n[INFO] Stopping Spark Streaming...")
    spark.stop()
    print("[INFO] Spark Streaming stopped")
```

---

### ğŸš€ BÆ°á»›c 6: Cháº¡y Spark Streaming

#### 6.1. Äáº£m báº£o cÃ¡c services Ä‘ang cháº¡y
```bash
# Kiá»ƒm tra táº¥t cáº£ services
jps

# Pháº£i tháº¥y:
# - QuorumPeerMain (Zookeeper)
# - Kafka
# - NameNode (HDFS)
# - DataNode (HDFS)

# Kiá»ƒm tra MongoDB
sudo systemctl status mongod
```

#### 6.2. Cháº¡y Spark Streaming
```bash
# BÆ°á»›c 1: Activate virtual environment
source ~/spark-venv/bin/activate

# BÆ°á»›c 2: Load biáº¿n mÃ´i trÆ°á»ng
source ~/.bashrc

# BÆ°á»›c 3: Cháº¡y Spark Streaming
spark-submit \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3,org.mongodb.spark:mongo-spark-connector_2.12:10.4.0 \
  --master local[*] \
  --driver-memory 2g \
  --executor-memory 2g \
  spark_streaming_consumer.py
```

**Káº¿t quáº£ mong Ä‘á»£i:**
```
[CONFIG] Kafka: 172.27.34.172:9092
[CONFIG] Topic: house-listings
[INFO] Spark Session initialized
[INFO] Connecting to Kafka...
[INFO] Connected to Kafka successfully
[INFO] Data cleaning configured
[INFO] Writing to HDFS: hdfs://localhost:9000/bigdata/house-listings/raw
[INFO] Writing aggregations to HDFS
[INFO] Writing to MongoDB configured

================================================================================
ğŸš€ Spark Streaming Started Successfully!
================================================================================
ğŸ“Š Monitoring:
   - Spark UI: http://localhost:4040
   - HDFS UI: http://localhost:9870
   - MongoDB: mongosh â†’ use bigdata_houses â†’ db.listings.find().limit(5)
================================================================================

â³ Waiting for data from Kafka topic 'house-listings'...
```

#### 6.3. Trong terminal khÃ¡c, cháº¡y Producer Ä‘á»ƒ gá»­i data
```bash
# Windows CMD/PowerShell
python kafka_producer.py
```

---

### ğŸ“Š BÆ°á»›c 7: Monitoring vÃ  Kiá»ƒm tra káº¿t quáº£

#### 7.1. Spark UI
Má»Ÿ trÃ¬nh duyá»‡t: http://localhost:4040
- Xem Streaming tab
- Kiá»ƒm tra Input Rate, Processing Time
- Xem cÃ¡c stages vÃ  tasks

#### 7.2. Kiá»ƒm tra dá»¯ liá»‡u trong HDFS
```bash
# Xem cáº¥u trÃºc thÆ° má»¥c
hdfs dfs -ls /bigdata/house-listings
hdfs dfs -ls /bigdata/house-listings/raw
hdfs dfs -ls /bigdata/house-listings/aggregations

# Äáº¿m sá»‘ files
hdfs dfs -count /bigdata/house-listings/raw

# Xem ná»™i dung file (láº¥y 1 file báº¥t ká»³)
hdfs dfs -cat /bigdata/house-listings/raw/part-*.parquet | head -100
```

#### 7.3. Kiá»ƒm tra dá»¯ liá»‡u trong MongoDB
```bash
mongosh
```

Trong MongoDB shell:
```javascript
use bigdata_houses

// Äáº¿m sá»‘ documents
db.listings.countDocuments()

// Xem 5 records má»›i nháº¥t
db.listings.find().sort({crawl_timestamp: -1}).limit(5).pretty()

// Thá»‘ng kÃª theo quáº­n
db.listings.aggregate([
  { $group: {
      _id: "$district",
      count: { $sum: 1 },
      avg_price: { $avg: "$price" },
      avg_area: { $avg: "$area_m2" }
  }},
  { $sort: { count: -1 } },
  { $limit: 10 }
])

// TÃ¬m nhÃ  giÃ¡ > 5 tá»·
db.listings.find({ price: { $gt: 5000000000 } }).limit(5).pretty()

exit
```

#### 7.4. Kiá»ƒm tra Kafka Consumer Group
```bash
# Xem consumer groups
kafka-consumer-groups.sh --bootstrap-server localhost:9092 --list

# Xem chi tiáº¿t lag cá»§a group
kafka-consumer-groups.sh \
  --bootstrap-server localhost:9092 \
  --describe \
  --group spark-kafka-streaming
```

---

### ğŸ› ï¸ Troubleshooting Stream Processing

#### Lá»—i: Checkpoint corrupt (Error reading delta file)

**NguyÃªn nhÃ¢n:** Spark Streaming bá»‹ dá»«ng Ä‘á»™t ngá»™t hoáº·c HDFS checkpoint bá»‹ lá»—i.

**Giáº£i phÃ¡p:**
```bash
# 1. Dá»«ng Spark Streaming (Ctrl+C)
# Äá»£i shutdown hoÃ n toÃ n (~10 giÃ¢y)

# 2. XÃ³a checkpoint
rm -rf /tmp/spark-checkpoints

# 3. Cháº¡y láº¡i Spark Streaming
spark-submit \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3,org.mongodb.spark:mongo-spark-connector_2.12:10.4.0 \
  --master local[*] \
  --driver-memory 2g \
  --executor-memory 2g \
  spark_streaming_consumer.py
```

**LÆ°u Ã½:** Script hiá»‡n dÃ¹ng local checkpoint (`file:///tmp/spark-checkpoints`) thay vÃ¬ HDFS Ä‘á»ƒ trÃ¡nh lá»—i nÃ y.

#### Lá»—i: MongoDB duplicate key (E11000)

**NguyÃªn nhÃ¢n:** Spark Ä‘á»c láº¡i data cÅ© tá»« Kafka vÃ  cá»‘ insert vÃ o MongoDB.

**Giáº£i phÃ¡p:** Script Ä‘Ã£ config upsert mode vá»›i `replaceDocument=true` vÃ  `idFieldList=id`. Náº¿u váº«n lá»—i:
```bash
# Option 1: XÃ³a data cÅ© trong MongoDB
mongosh
use bigdata_houses
db.listings.deleteMany({})
exit

# Option 2: XÃ³a checkpoint vÃ  cháº¡y láº¡i
rm -rf /tmp/spark-checkpoints
```

#### Lá»—i: HDFS khÃ´ng khá»Ÿi Ä‘á»™ng (NameNode/DataNode khÃ´ng xuáº¥t hiá»‡n trong jps)

**NguyÃªn nhÃ¢n:** File `core-site.xml` chÆ°a cáº¥u hÃ¬nh hoáº·c cáº¥u hÃ¬nh sai.

**Giáº£i phÃ¡p:**
```bash
# 1. Xem log Ä‘á»ƒ tÃ¬m lá»—i
tail -50 /usr/local/hadoop/logs/hadoop-*-namenode-*.log

# 2. Náº¿u tháº¥y lá»—i "Invalid URI for NameNode address (check fs.defaultFS): file:/// has no authority"
# Kiá»ƒm tra vÃ  sá»­a core-site.xml (xem BÆ°á»›c 4.0.B)

# 3. Format láº¡i NameNode
hdfs namenode -format -force

# 4. Start láº¡i HDFS
start-dfs.sh
jps
```

#### Lá»—i: SSH connection refused khi start HDFS

**NguyÃªn nhÃ¢n:** SSH service chÆ°a cháº¡y hoáº·c chÆ°a cáº¥u hÃ¬nh passwordless SSH.

**Giáº£i phÃ¡p:**
```bash
# 1. Start SSH service
sudo service ssh start

# 2. Kiá»ƒm tra SSH
sudo service ssh status

# 3. Náº¿u chÆ°a cÃ³ SSH key, táº¡o má»›i (xem BÆ°á»›c 4.0.A)
ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
chmod 0600 ~/.ssh/authorized_keys

# 4. Test SSH
ssh localhost
```

#### Lá»—i: Kafka topic bá»‹ máº¥t sau khi restart

**NguyÃªn nhÃ¢n:** Kafka data directory máº·c Ä‘á»‹nh á»Ÿ `/tmp/kafka-logs` cÃ³ thá»ƒ bá»‹ xÃ³a khi reboot.

**Giáº£i phÃ¡p - Äá»•i Kafka data directory sang vá»‹ trÃ­ cá»‘ Ä‘á»‹nh:**
```bash
# 1. Táº¡o thÆ° má»¥c lÆ°u trá»¯ vÄ©nh viá»…n
sudo mkdir -p /usr/local/kafka/data
sudo chown -R $USER:$USER /usr/local/kafka/data

# 2. Sá»­a file config
nano /usr/local/kafka/config/server.properties

# 3. TÃ¬m dÃ²ng: log.dirs=/tmp/kafka-logs
# Thay báº±ng: log.dirs=/usr/local/kafka/data

# 4. TÆ°Æ¡ng tá»± cho Zookeeper
sudo mkdir -p /usr/local/kafka/zookeeper-data
sudo chown -R $USER:$USER /usr/local/kafka/zookeeper-data

nano /usr/local/kafka/config/zookeeper.properties
# TÃ¬m: dataDir=/tmp/zookeeper
# Thay: dataDir=/usr/local/kafka/zookeeper-data

# 5. Restart Kafka cluster
bin/kafka-server-stop.sh
bin/zookeeper-server-stop.sh
sleep 5
bin/zookeeper-server-start.sh config/zookeeper.properties &
sleep 5
bin/kafka-server-start.sh config/server.properties &

# 6. Táº¡o láº¡i topic (chá»‰ láº§n Ä‘áº§u sau khi Ä‘á»•i directory)
kafka-topics.sh --create --topic house-listings \
  --bootstrap-server localhost:9092 \
  --partitions 3 \
  --replication-factor 1
```

**Sau khi cáº¥u hÃ¬nh, topic sáº½ persistent vÄ©nh viá»…n.**

#### Lá»—i: Kafka connection timeout
```bash
# Kiá»ƒm tra Kafka Ä‘ang cháº¡y
jps | grep Kafka

# Kiá»ƒm tra topic
kafka-topics.sh --list --bootstrap-server localhost:9092

# Test connection
nc -zv localhost 9092
```

#### Lá»—i: HDFS connection refused
```bash
# Kiá»ƒm tra HDFS
jps | grep -E "NameNode|DataNode"

# Khá»Ÿi Ä‘á»™ng HDFS náº¿u cáº§n
start-dfs.sh

# Kiá»ƒm tra safe mode
hdfs dfsadmin -safemode get

# ThoÃ¡t safe mode náº¿u stuck
hdfs dfsadmin -safemode leave
```

#### Lá»—i: MongoDB connection failed
```bash
# Kiá»ƒm tra MongoDB status
sudo systemctl status mongod

# Khá»Ÿi Ä‘á»™ng náº¿u cáº§n
sudo systemctl start mongod

# Xem logs
sudo tail -f /var/log/mongodb/mongod.log
```

#### Lá»—i: OutOfMemoryError trong Spark
```bash
# TÄƒng memory khi cháº¡y spark-submit
spark-submit \
  --driver-memory 4g \
  --executor-memory 4g \
  ...
```

#### Xem Spark Streaming logs
```bash
# Logs Ä‘Æ°á»£c ghi vÃ o console
# Hoáº·c xem trong Spark UI: http://localhost:4040
```

---

### ğŸ“ˆ NÃ¢ng cao: ThÃªm Aggregations vÃ  Analytics

ThÃªm vÃ o file `spark_streaming_consumer.py`:

```python
# Aggregation theo price_category
price_stats = cleaned_df \
    .groupBy(
        window(col("processing_time"), "10 minutes"),
        col("price_category")
    ) \
    .agg(
        count("*").alias("count"),
        avg("price").alias("avg_price")
    )

# Aggregation theo region
region_stats = cleaned_df \
    .groupBy(
        window(col("processing_time"), "10 minutes"),
        col("region")
    ) \
    .agg(
        count("*").alias("total"),
        avg("price").alias("avg_price"),
        avg("area_m2").alias("avg_area")
    )

# Write to separate paths
price_stats.writeStream \
    .outputMode("complete") \
    .format("parquet") \
    .option("path", f"{HDFS_PATH}/stats_price") \
    .option("checkpointLocation", f"{CHECKPOINT_PATH}/stats_price") \
    .start()

region_stats.writeStream \
    .outputMode("complete") \
    .format("parquet") \
    .option("path", f"{HDFS_PATH}/stats_region") \
    .option("checkpointLocation", f"{CHECKPOINT_PATH}/stats_region") \
    .start()
```

---

## ğŸ”„ Quáº£n lÃ½ Services

### Thá»© tá»± khá»Ÿi Ä‘á»™ng Services

**Thá»© tá»± Ä‘Ãºng khi báº­t táº¥t cáº£ services:**

```bash
# 1. SSH service (náº¿u chÆ°a cháº¡y)
sudo service ssh start

# 2. Zookeeper (pháº£i cháº¡y trÆ°á»›c Kafka)
cd /usr/local/kafka
bin/zookeeper-server-start.sh config/zookeeper.properties &

# 3. Kafka (terminal má»›i, sau khi Zookeeper Ä‘Ã£ cháº¡y á»•n Ä‘á»‹nh ~5s)
sleep 5
cd /usr/local/kafka
bin/kafka-server-start.sh config/server.properties &

# 4. HDFS (terminal má»›i)
start-dfs.sh

# 5. MongoDB
sudo systemctl start mongod

# 6. Kiá»ƒm tra táº¥t cáº£ services
jps
sudo systemctl status mongod
```

### Thá»© tá»± táº¯t Services (ngÆ°á»£c láº¡i vá»›i thá»© tá»± báº­t)

**CÃ¡ch táº¯t an toÃ n:**

```bash
# 1. Dá»«ng Spark Streaming (náº¿u Ä‘ang cháº¡y)
# Trong terminal Ä‘ang cháº¡y spark-submit, nháº¥n Ctrl+C

# 2. Dá»«ng HDFS
stop-dfs.sh

# 3. Dá»«ng MongoDB
sudo systemctl stop mongod

# 4. Dá»«ng Kafka
cd /usr/local/kafka
bin/kafka-server-stop.sh

# 5. Dá»«ng Zookeeper (cuá»‘i cÃ¹ng)
bin/zookeeper-server-stop.sh

# 6. Kiá»ƒm tra Ä‘Ã£ táº¯t háº¿t chÆ°a
jps
# Chá»‰ tháº¥y Jps lÃ  OK
```

### Kiá»ƒm tra tráº¡ng thÃ¡i Services

```bash
# Kiá»ƒm tra táº¥t cáº£ Java processes
jps
# Output mong Ä‘á»£i khi Ä‘ang cháº¡y:
# - QuorumPeerMain (Zookeeper)
# - Kafka
# - NameNode
# - DataNode
# - SecondaryNameNode

# Kiá»ƒm tra MongoDB
sudo systemctl status mongod

# Kiá»ƒm tra SSH
sudo service ssh status

# Kiá»ƒm tra port Ä‘ang Ä‘Æ°á»£c sá»­ dá»¥ng
netstat -tuln | grep -E "2181|9092|9000|9870|27017"
# 2181  - Zookeeper
# 9092  - Kafka
# 9000  - HDFS
# 9870  - HDFS Web UI
# 27017 - MongoDB
```

### Khá»Ÿi Ä‘á»™ng láº¡i má»™t Service cá»¥ thá»ƒ

#### Khá»Ÿi Ä‘á»™ng láº¡i Kafka
```bash
# Dá»«ng
cd /usr/local/kafka
bin/kafka-server-stop.sh

# Äá»£i ~5 giÃ¢y
sleep 5

# Báº­t láº¡i
bin/kafka-server-start.sh config/server.properties &
```

#### Khá»Ÿi Ä‘á»™ng láº¡i Zookeeper
```bash
# LÆ°u Ã½: Náº¿u restart Zookeeper, pháº£i restart Kafka sau Ä‘Ã³

# Dá»«ng Kafka trÆ°á»›c
cd /usr/local/kafka
bin/kafka-server-stop.sh

# Dá»«ng Zookeeper
bin/zookeeper-server-stop.sh

# Äá»£i ~5 giÃ¢y
sleep 5

# Báº­t láº¡i Zookeeper
bin/zookeeper-server-start.sh config/zookeeper.properties &

# Äá»£i Zookeeper á»•n Ä‘á»‹nh
sleep 5

# Báº­t láº¡i Kafka
bin/kafka-server-start.sh config/server.properties &
```

#### Khá»Ÿi Ä‘á»™ng láº¡i HDFS
```bash
# Dá»«ng
stop-dfs.sh

# Báº­t láº¡i
start-dfs.sh

# Kiá»ƒm tra
jps
hdfs dfs -ls /
```

#### Khá»Ÿi Ä‘á»™ng láº¡i MongoDB
```bash
# Dá»«ng
sudo systemctl stop mongod

# Báº­t láº¡i
sudo systemctl start mongod

# Kiá»ƒm tra
sudo systemctl status mongod
```

### Script tá»± Ä‘á»™ng (tÃ¹y chá»n)

Táº¡o file `start_all.sh`:
```bash
#!/bin/bash
echo "ğŸš€ Starting all services..."

# SSH
sudo service ssh start
echo "âœ… SSH started"

# Zookeeper
cd /usr/local/kafka
nohup bin/zookeeper-server-start.sh config/zookeeper.properties > /tmp/zookeeper.log 2>&1 &
echo "âœ… Zookeeper started"
sleep 5

# Kafka
nohup bin/kafka-server-start.sh config/server.properties > /tmp/kafka.log 2>&1 &
echo "âœ… Kafka started"
sleep 3

# HDFS
start-dfs.sh
echo "âœ… HDFS started"

# MongoDB
sudo systemctl start mongod
echo "âœ… MongoDB started"

echo ""
echo "ğŸ“Š Services status:"
jps
echo ""
sudo systemctl status mongod --no-pager
```

Táº¡o file `stop_all.sh`:
```bash
#!/bin/bash
echo "ğŸ›‘ Stopping all services..."

# HDFS
stop-dfs.sh
echo "âœ… HDFS stopped"

# MongoDB
sudo systemctl stop mongod
echo "âœ… MongoDB stopped"

# Kafka
cd /usr/local/kafka
bin/kafka-server-stop.sh
echo "âœ… Kafka stopped"
sleep 3

# Zookeeper
bin/zookeeper-server-stop.sh
echo "âœ… Zookeeper stopped"

echo ""
echo "ğŸ“Š Remaining processes:"
jps
```

PhÃ¢n quyá»n thá»±c thi:
```bash
chmod +x start_all.sh stop_all.sh
```

Sá»­ dá»¥ng:
```bash
# Báº­t táº¥t cáº£
./start_all.sh

# Táº¯t táº¥t cáº£
./stop_all.sh
```

---

### âœ… Checklist Stream Processing

- [ ] CÃ i Ä‘áº·t Apache Spark
- [ ] Cáº¥u hÃ¬nh biáº¿n mÃ´i trÆ°á»ng (SPARK_HOME, PATH)
- [ ] CÃ i Ä‘áº·t MongoDB
- [ ] Khá»Ÿi Ä‘á»™ng HDFS vÃ  táº¡o thÆ° má»¥c
- [ ] Táº¡o database vÃ  collection trong MongoDB
- [ ] Táº¡o file `spark_streaming_consumer.py`
- [ ] Cháº¡y Spark Streaming
- [ ] Cháº¡y Producer Ä‘á»ƒ test
- [ ] Kiá»ƒm tra Spark UI (http://localhost:4040)
- [ ] Kiá»ƒm tra dá»¯ liá»‡u trong HDFS
- [ ] Kiá»ƒm tra dá»¯ liá»‡u trong MongoDB
- [ ] Verify khÃ´ng cÃ²n lá»—i checkpoint/duplicate

### ğŸ¯ Káº¿t quáº£ mong Ä‘á»£i

**Spark Streaming cháº¡y thÃ nh cÃ´ng khi tháº¥y:**
```
[SUCCESS] Batch 0: Processed X records to MongoDB (upsert mode)
[SUCCESS] Batch 1: Processed X records to MongoDB (upsert mode)
```

**KhÃ´ng cÃ³ lá»—i:**
- âŒ Checkpoint corrupt errors
- âŒ E11000 duplicate key errors
- âŒ HDFS connection refused

**Warnings cÃ³ thá»ƒ ignore:**
- âš ï¸ `Unable to load native-hadoop library` (bÃ¬nh thÆ°á»ng trÃªn WSL2)
- âš ï¸ `CaseInsensitiveStringMap: Converting duplicated key` (khÃ´ng áº£nh hÆ°á»Ÿng)
- âš ï¸ `Current batch is falling behind` (bÃ¬nh thÆ°á»ng vá»›i batch Ä‘áº§u tiÃªn)

### ğŸ“ LÆ°u Ã½ quan trá»ng

1. **startingOffsets="latest"**: Chá»‰ xá»­ lÃ½ data Má»šI, trÃ¡nh Ä‘á»c láº¡i data cÅ©
2. **Local checkpoint**: DÃ¹ng `file:///tmp/spark-checkpoints` thay vÃ¬ HDFS (á»•n Ä‘á»‹nh hÆ¡n)
3. **MongoDB upsert**: Config `replaceDocument=true` Ä‘á»ƒ tá»± Ä‘á»™ng update thay vÃ¬ lá»—i duplicate
4. **KhÃ´ng dÃ¹ng dropDuplicates()**: GÃ¢y lá»—i checkpoint, dÃ¹ng MongoDB unique index thay tháº¿
5. **Dá»«ng Spark an toÃ n**: Nháº¥n Ctrl+C má»™t láº§n vÃ  Ä‘á»£i shutdown hoÃ n toÃ n (trÃ¡nh corrupt checkpoint)

---
